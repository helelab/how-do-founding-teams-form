{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spotting Patterns in Founding Team Formations Using an LSTM Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process through which entrepreneurial founding teams 'reach the starting line' has become a subject of academic study and interest in recent years. Building upon existing academic literature on venture team formation, Professors David Clough and Balagopal Vissa developed a theoretical model which provides novel insights on how a set of cofounders come together to form the initial founding team [1].\n",
    "\n",
    "In short, the work developed by the authors is grounded on the following pillars:\n",
    "\n",
    "1) Disaggregating the cofounder searching and evaluation process into separate entities to analyze in more depth how and when task competence and interpersonal fit affect the founding team formation process.\n",
    "\n",
    "2) Conceiving founding team formation as a dynamic choice process that takes a middle stance between the more recent \"agentic view\" (i.e. entrepreneurs acting strategically as \"players\" within their networks) and the more traditional structuralist approach (i.e. entrepreneurs as passive occupants of social structural positions).\n",
    "\n",
    "3) Using a process-focused perspective to depict different founding team formation pathways, distilling the factors that drive both successful attempts in founding team formation and the subsequent churn when cofounder entry/exit dynamics emerge in the founding team.\n",
    "\n",
    "Looking to construct an experiment upon which the model's key assumptions and prepositions could be tested on, we decided to develop an LSTM Neural Network that uses an Natural Language Processing (NLP) model to process testimonies shared by entrepreneurs about their experiences on founding team formation. Several paragraphs were scraped from open forums websites such as Quora and entrepreneurial websites such as Y Combinator, looking to build a training set that resembles how entrepreneurs express themselves in real-life settings when faced with the question on how they met their co-founders.\n",
    "\n",
    "Here's an example of an answer provided by a Quora user on the question \"How did you meet your co-founder?\"\n",
    "\n",
    "<img src=\"images/quora.png\">\n",
    "\n",
    "To make the text \"machine readable\", the testimonies were broken down into individual sentences and subsequently turned into vectors of indices through a Python function. Once the sentences have been correctly set to work as inputs for the LSTM Neural Network, a layer of pre-trained vectors is loaded into the model using the GloVe 50-dimensional model developed by the Stanford NLP group (https://nlp.stanford.edu/projects/glove/).\n",
    "\n",
    "Finally, a 2-layer LSTM Neural Network with an additional hidden layer and a Softmax activation function is configured using the Keras Framework to process the inputs and classify the sentences according to the class definition described below:\n",
    "\n",
    "<b>Class [0]:</b> The entrepreneur gives (or has given) more weight to elements like chemistry, trust and personal bonding when evaluating a prospective cofounder.\n",
    "\n",
    "<b>Class [1]:</b> The entrepreneur gives (or has given) more weight to elements like technical competences, past experience and execution capabilities when evaluating a prospective cofounder.\n",
    "\n",
    "<b>Class [2]:</b> The entrepreneur looks (or has looked) for connections with whom she has strong ties when looking out for a potential cofounder (Serendipitous FOCIs).\n",
    "\n",
    "<b>Class [3]:</b> The entrepreneur actively looks (or has looked) within her professional networks and/or engages in a dedicated search when looking out for a potential cofounder (Goal-directed FOCIs).\n",
    "\n",
    "These classes were developed taking into account propositions #1, #2, #3 and #4 as described in the research paper, narrowing down our focus to the process followed by founders when they \"search for and mutually evaluate\" one another. Each sentence uploaded into the training set was carefully labelled to accurately represent the most appropriate class. Given that some sentences could be categorized into two different classes based on subjective claims by the human annotator (i.e. the sentence \"Evaluate whether you are able to work well together\" may be classified as both [0] and [1]), we estimate a human error level of ≈ 10%.\n",
    "\n",
    "So far, the model has been able to correctly identify ≈ 97% of the examples in the training set (n = 938), and ≈ 77% of the examples in the test set (n = 44, accounting for data the model has never \"seen\"), which implies some work has yet to be done to chop down overfitting.\n",
    "\n",
    "This model has the potential to provide a reliable framework upon which experiments at big scales can be conducted, looking to test the principal hypotheses and prepositions exposed by the conceptual framework. Concretely, the model could help to advance the theoretical research developed by Prof. Clough and Prof. Vissa by using an NLP model to systematically web-scrape, classify and spot patterns among founder testimonies and evaluate them according to the stated prepositions.\n",
    "\n",
    "<b>References:</b>\n",
    "\n",
    "[1] Clough, David and Vissa, Balagopal, How Do Founding Teams Form? Towards a Behavioral Theory of Founding Team Formation (July 2, 2018). INSEAD Working Paper No. 2018/26/EFE. Available at SSRN: https://ssrn.com/abstract=3206701 or http://dx.doi.org/10.2139/ssrn.3206701"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structure of the LSTM Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading key Python libraries and Deep Learning Frameworks (Keras):\n",
    "\n",
    "import numpy as np\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Dropout, LSTM, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.initializers import glorot_uniform\n",
    "from utils_v1 import *\n",
    "\n",
    "# Loading of data files: (explain here the glove 6B model)\n",
    "\n",
    "word_to_index, index_to_word, word_to_vec_map = read_glove_vecs('data/glove.6B.50d.txt')\n",
    "\n",
    "X, Y = read_csv('data/sentences_data_v5.csv')\n",
    "X_train, Y_train = read_csv('data/sentences_data_train_v5.csv')\n",
    "X_test, Y_test = read_csv('data/sentences_data_test_v5.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data cleansing process - eliminating characters that may cause a KeyError message below:\n",
    "\n",
    "X[0] = X[0].replace('\\ufeff', '')\n",
    "X_train[0] = X_train[0].replace('\\ufeff', '')\n",
    "X_test[0] = X_test[0].replace('\\ufeff', '')\n",
    "\n",
    "# Finding out the max length of any given sentence in the dataset (counting in words) and storing the value in a variable called maxLen\n",
    "\n",
    "maxLen = len(max(X, key=len).split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentences_to_indices(X, word_to_index, max_len):\n",
    "    \"\"\"\n",
    "    Converts an array of sentences (strings) into an array of indices corresponding to words in the sentences.\n",
    "    The output shape should be such that it can be given to `Embedding()`\n",
    "    \n",
    "    Arguments:\n",
    "    X -- array of sentences (strings), of shape (m, 1)\n",
    "    word_to_index -- a dictionary containing all words mapped to their corresponding indeces\n",
    "    max_len -- maximum number of words in a sentence. Gets plugged as maxLen into the function. \n",
    "    \n",
    "    Returns:\n",
    "    X_indices -- array of indices corresponding to words in the sentences from X, of shape (m, max_len)\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[0]                                   # number of training examples\n",
    "    # Initializes X_indices as a numpy matrix of zeros and the correct shape\n",
    "    \n",
    "    X_indices = np.zeros((m, max_len))\n",
    "    \n",
    "    for i in range(m):                               # Creates a loop over training examples\n",
    "        # Converts the ith training sentence in lower case and split is into words. You should get a list of words.\n",
    "        \n",
    "        sentence_words = [word.lower().replace('\\t', '') for word in X[i].split(' ') if word.replace('\\t', '') != '']\n",
    "        # Initializes j to 0\n",
    "        \n",
    "        j = 0\n",
    "        # Loop over the words of sentence_words\n",
    "        \n",
    "        for w in sentence_words:\n",
    "            \n",
    "            # Set the (i,j)th entry of X_indices to the index of the correct word.\n",
    "            X_indices[i, j] = word_to_index[w]\n",
    "            \n",
    "            # Increment j to j + 1\n",
    "            j = j + 1\n",
    "    \n",
    "    return X_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[185457. 170714. 209675. ...      0.      0.      0.]\n",
      " [368321. 268046. 254258. ...      0.      0.      0.]\n",
      " [ 56041. 103640.  54718. ...      0.      0.      0.]\n",
      " ...\n",
      " [366254. 239792. 335202. ...      0.      0.      0.]\n",
      " [224573. 259594. 141855. ...      0.      0.      0.]\n",
      " [357640.  58997. 233708. ...      0.      0.      0.]]\n"
     ]
    }
   ],
   "source": [
    "indices = sentences_to_indices(X, word_to_index, 18)\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrained_embedding_layer(word_to_vec_map, word_to_index):\n",
    "    \"\"\"\n",
    "    Creates a Keras Embedding() layer and loads in pre-trained GloVe 50-dimensional vectors.\n",
    "    \n",
    "    Arguments:\n",
    "    word_to_vec_map -- dictionary mapping words to their GloVe vector representation.\n",
    "    word_to_index -- dictionary mapping from words to their indices in the vocabulary (400,001 words)\n",
    "\n",
    "    Returns:\n",
    "    embedding_layer -- pretrained layer Keras instance\n",
    "    \"\"\"\n",
    "    \n",
    "    vocab_len = len(word_to_index) + 1                    # adding 1 to fit Keras embedding (requirement)\n",
    "    emb_dim = word_to_vec_map[\"co-founder\"].shape[0]      # defines dimensionality of GloVe word vectors (= 50)\n",
    "    \n",
    "    # Initializes the embedding matrix as a numpy array of zeros of shape (vocab_len, dimensions of word vectors = emb_dim)\n",
    "    emb_matrix = np.zeros((vocab_len, emb_dim))\n",
    "    \n",
    "    # Sets each row \"index\" of the embedding matrix to be the word vector representation of the \"index\"th word of the vocabulary\n",
    "    for word, index in word_to_index.items():\n",
    "        emb_matrix[index, :] = word_to_vec_map[word]\n",
    "\n",
    "    # Defines the Keras embedding layer with the correct output/input sizes\n",
    "    embedding_layer = Embedding(vocab_len, emb_dim, trainable = True)\n",
    "\n",
    "    # Builds the embedding layer, which is required before setting the weights of the embedding layer.\n",
    "    embedding_layer.build((None,))\n",
    "    \n",
    "    # Sets the weights of the embedding layer to the embedding matrix. The layer should now be pretrained.\n",
    "    embedding_layer.set_weights([emb_matrix])\n",
    "    \n",
    "    return embedding_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NLP_model_v1(input_shape, word_to_vec_map, word_to_index):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function creating the NLP model's graph.\n",
    "    \n",
    "    Arguments:\n",
    "    input_shape -- shape of the input, usually (max_len,)\n",
    "    word_to_vec_map -- dictionary mapping every word in a vocabulary into its 50-dimensional vector representation\n",
    "    word_to_index -- dictionary mapping from words to their indices in the vocabulary (400,001 words)\n",
    "\n",
    "    Returns:\n",
    "    model -- a model instance in Keras\n",
    "    \"\"\"\n",
    "    \n",
    "    # Defines sentence_indices as the input of the graph, it should be of shape input_shape and dtype 'int32' (as it contains indices)\n",
    "    sentence_indices = Input(shape = input_shape, dtype = 'int32')\n",
    "    \n",
    "    # Creates the embedding layer pretrained with GloVe Vectors\n",
    "    embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)\n",
    "    \n",
    "    # Propagates sentence_indices through the embedding layer, getting back the embeddings\n",
    "    embeddings = embedding_layer(sentence_indices)  \n",
    "    \n",
    "    # Propagates the embeddings through an LSTM layer with 80-dimensional hidden state, setting return_sequences = True to get back a batch of sequences\n",
    "    X = LSTM(60, return_sequences = True)(embeddings)\n",
    "    \n",
    "    # Adds dropout with a probability of 0.5 to regularize the network\n",
    "    X = Dropout(0.5)(X)\n",
    "    \n",
    "    # Propagates X trough another LSTM layer with 80-dimensional hidden state, this time with the returned output as a single hidden state, not a batch of sequences\n",
    "    X = LSTM(60, return_sequences = False)(X)\n",
    "    \n",
    "    # Adds dropout with a probability of 0.5 to regularize the network\n",
    "    X = Dropout(0.5)(X)\n",
    "    \n",
    "    # Propagates X through a Dense layer with 4 units to get back a batch of 4-dimensional vectors\n",
    "    X = Dense(4, activation = None)(X)\n",
    "    \n",
    "    # Adds a softmax activation to estimate the probablity of each output \n",
    "    X = Activation('softmax')(X)\n",
    "    \n",
    "    # Creates the Model instance which converts sentence_indices into X.\n",
    "    model_v1 = Model(sentence_indices, X)\n",
    "    \n",
    "    return model_v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_7 (InputLayer)         (None, 18)                0         \n",
      "_________________________________________________________________\n",
      "embedding_7 (Embedding)      (None, 18, 50)            20000050  \n",
      "_________________________________________________________________\n",
      "lstm_13 (LSTM)               (None, 18, 60)            26640     \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 18, 60)            0         \n",
      "_________________________________________________________________\n",
      "lstm_14 (LSTM)               (None, 60)                29040     \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 60)                0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 4)                 244       \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 4)                 0         \n",
      "=================================================================\n",
      "Total params: 20,055,974\n",
      "Trainable params: 20,055,974\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_v1 = NLP_model_v1((18,), word_to_vec_map, word_to_index)\n",
    "model_v1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiles the model using loss = 'categorical crossentropy' as we are dealing with a multi-class optmimization problem. Sets 'adam' as the optimizer method.\n",
    "model_v1.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns X_indices from the sentences_to_indices function and saves it in X_train_indices\n",
    "X_train_indices = sentences_to_indices(X_train, word_to_index, 18)\n",
    "\n",
    "# Applies One-hot vectorization to the training set Y_train, stores values in Y_train_oh. We use C = 4 as we are looking to get a probability distribution spread across 4 different clases. \n",
    "Y_train_oh = convert_to_one_hot(Y_train, C = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "938/938 [==============================] - 13s 14ms/step - loss: 1.3761 - acc: 0.3081\n",
      "Epoch 2/25\n",
      "938/938 [==============================] - 11s 11ms/step - loss: 1.2058 - acc: 0.4606\n",
      "Epoch 3/25\n",
      "938/938 [==============================] - 11s 12ms/step - loss: 1.0666 - acc: 0.5299\n",
      "Epoch 4/25\n",
      "938/938 [==============================] - 11s 11ms/step - loss: 0.9225 - acc: 0.6258\n",
      "Epoch 5/25\n",
      "938/938 [==============================] - 10s 11ms/step - loss: 0.7982 - acc: 0.6962\n",
      "Epoch 6/25\n",
      "938/938 [==============================] - 10s 10ms/step - loss: 0.7091 - acc: 0.7196\n",
      "Epoch 7/25\n",
      "938/938 [==============================] - 10s 11ms/step - loss: 0.5912 - acc: 0.7676\n",
      "Epoch 8/25\n",
      "938/938 [==============================] - 10s 11ms/step - loss: 0.5020 - acc: 0.8252\n",
      "Epoch 9/25\n",
      "938/938 [==============================] - 10s 11ms/step - loss: 0.3988 - acc: 0.8721\n",
      "Epoch 10/25\n",
      "938/938 [==============================] - 9s 10ms/step - loss: 0.3451 - acc: 0.8827\n",
      "Epoch 11/25\n",
      "938/938 [==============================] - 10s 10ms/step - loss: 0.3608 - acc: 0.8806\n",
      "Epoch 12/25\n",
      "938/938 [==============================] - 10s 10ms/step - loss: 0.2504 - acc: 0.9318\n",
      "Epoch 13/25\n",
      "938/938 [==============================] - 10s 10ms/step - loss: 0.2396 - acc: 0.9200\n",
      "Epoch 14/25\n",
      "938/938 [==============================] - 10s 10ms/step - loss: 0.1789 - acc: 0.9435\n",
      "Epoch 15/25\n",
      "938/938 [==============================] - 10s 11ms/step - loss: 0.1528 - acc: 0.9584\n",
      "Epoch 16/25\n",
      "938/938 [==============================] - 11s 11ms/step - loss: 0.1543 - acc: 0.9584\n",
      "Epoch 17/25\n",
      "938/938 [==============================] - 11s 12ms/step - loss: 0.1196 - acc: 0.9691\n",
      "Epoch 18/25\n",
      "938/938 [==============================] - 12s 12ms/step - loss: 0.0822 - acc: 0.9819\n",
      "Epoch 19/25\n",
      "938/938 [==============================] - 10s 11ms/step - loss: 0.0732 - acc: 0.9829\n",
      "Epoch 20/25\n",
      "938/938 [==============================] - 11s 11ms/step - loss: 0.0783 - acc: 0.9819\n",
      "Epoch 21/25\n",
      "938/938 [==============================] - 10s 11ms/step - loss: 0.0898 - acc: 0.9765\n",
      "Epoch 22/25\n",
      "938/938 [==============================] - 10s 11ms/step - loss: 0.0608 - acc: 0.9872\n",
      "Epoch 23/25\n",
      "938/938 [==============================] - 10s 10ms/step - loss: 0.0353 - acc: 0.9936\n",
      "Epoch 24/25\n",
      "938/938 [==============================] - 10s 11ms/step - loss: 0.0364 - acc: 0.9915\n",
      "Epoch 25/25\n",
      "938/938 [==============================] - 10s 10ms/step - loss: 0.0569 - acc: 0.9851\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a2f3a6da0>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fits the Neural Network model using Keras\n",
    "model_v1.fit(X_train_indices, Y_train_oh, epochs = 25, batch_size = 32, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44/44 [==============================] - 1s 28ms/step\n",
      "\n",
      "Test_v1 accuracy =  0.7272727326913313\n"
     ]
    }
   ],
   "source": [
    "# Evalutes the model accuracy on a test set with 44 training examples\n",
    "\n",
    "X_test_indices = sentences_to_indices(X_test, word_to_index, max_len = 18)\n",
    "Y_test_oh = convert_to_one_hot(Y_test, C = 4)\n",
    "loss_v1, acc_v1 = model_v1.evaluate(X_test_indices, Y_test_oh)\n",
    "print()\n",
    "print(\"Test_v1 accuracy = \", acc_v1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n",
      "[[9.9787736e-01 1.5828023e-03 4.3174106e-04 1.0811740e-04]]\n"
     ]
    }
   ],
   "source": [
    "# Tests the model with sentences that were not included in the Training and Test sets.\n",
    "\n",
    "x_test = np.array(['Chemistry for me is a key attribute'])\n",
    "X_test_indices = sentences_to_indices(x_test, word_to_index, 18)\n",
    "test_v1a = np.argmax(model_v1.predict(X_test_indices), axis = 1)\n",
    "test_v1b = model_v1.predict(X_test_indices)\n",
    "print(test_v1a)\n",
    "print(test_v1b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3]\n",
      "[[9.9803416e-05 1.0860464e-04 8.0783782e-04 9.9898368e-01]]\n"
     ]
    }
   ],
   "source": [
    "x_test = np.array(['I joined an accelerator program looking for a cofounder'])\n",
    "X_test_indices = sentences_to_indices(x_test, word_to_index, 18)\n",
    "test_v1a = np.argmax(model_v1.predict(X_test_indices), axis = 1)\n",
    "test_v1b = model_v1.predict(X_test_indices)\n",
    "print(test_v1a)\n",
    "print(test_v1b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n",
      "[[9.9689329e-01 2.2917457e-03 6.5543782e-04 1.5956673e-04]]\n"
     ]
    }
   ],
   "source": [
    "x_test = np.array(['I think she will make a good work'])\n",
    "X_test_indices = sentences_to_indices(x_test, word_to_index, 18)\n",
    "test_v1a = np.argmax(model_v1.predict(X_test_indices), axis = 1)\n",
    "test_v1b = model_v1.predict(X_test_indices)\n",
    "print(test_v1a)\n",
    "print(test_v1b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n",
      "[[9.4671600e-04 9.9900025e-01 8.9602026e-06 4.4115735e-05]]\n"
     ]
    }
   ],
   "source": [
    "x_test = np.array(['My cofounder should be able to execute the companys vision'])\n",
    "X_test_indices = sentences_to_indices(x_test, word_to_index, 18)\n",
    "test_v1a = np.argmax(model_v1.predict(X_test_indices), axis = 1)\n",
    "test_v1b = model_v1.predict(X_test_indices)\n",
    "print(test_v1a)\n",
    "print(test_v1b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2]\n",
      "[[7.4918424e-03 1.1428070e-03 9.9079996e-01 5.6543708e-04]]\n"
     ]
    }
   ],
   "source": [
    "x_test = np.array(['My spouse and I founded a startup together'])\n",
    "X_test_indices = sentences_to_indices(x_test, word_to_index, 18)\n",
    "test_v1a = np.argmax(model_v1.predict(X_test_indices), axis = 1)\n",
    "test_v1b = model_v1.predict(X_test_indices)\n",
    "print(test_v1a)\n",
    "print(test_v1b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
