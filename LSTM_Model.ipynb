{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spotting Patterns in Founding Team Formations Using an LSTM Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process through which entrepreneurial founding teams 'reach the starting line' has become a subject of academic study and interest in recent years. Building upon existing academic literature on venture team formation, Professors David Clough and Balagopal Vissa developed a theoretical model which provides novel insights on how a set of cofounders come together to form the intial founding team [1].\n",
    "\n",
    "In short, the work developed by the authors is grounded on the following pillars:\n",
    "\n",
    "1) Disaggregating the cofounder searching and evaluation process into separate entities to analyze in more depth how and when task competence and interpersonal fit affect the founding team formation process.   \n",
    "\n",
    "2) Conceiving founding team formation as a dynamic choice process that takes a middle stance between the more recent \"agentic view\" (i.e. entrepreneurs acting strategically as \"players\" within their networks) and the more traditional structuralist approach (i.e. entrepreneurs as passive occupants of social structural positions).\n",
    "\n",
    "3) Using a process-focused perspective to depict different founding team formation pathways, distilling the factors that drive both successful attempts in founding team formation and the subsequent churn when cofounder entry/exit dynamics emerge in the founding team. \n",
    "\n",
    "Looking to construct an experiment upon which the model's key assumptions and prepositions could be tested on, we decided to develop an LSTM Neural Network that uses an NLP model to process testimonies shared by entrepreneurs about their experiences on founding team formation. Several paragraphs were scraped from open forums websites such as Quora and entreprenurial websites such as Y Combinator looking to build a training set that resembles how entrepreneurs express themselves in real-life settings. \n",
    "\n",
    "To make the text \"machine readable\", the testimonies are broken down into individual sentences and subsequently turned into vectors of indices. Once the sentences have been correctly set to work as inputs for the LSTM Neural Network, a layer of pre-trained vectors is loaded into the model using the GloVe 50-dimensional developed by the Stanford NLP group (https://nlp.stanford.edu/projects/glove/).\n",
    "\n",
    "Finally, a 2-layer LSTM Neural Network with an additional hidden layer and a Softmax activation function is configured using the Keras Framework to process the inputs and classify the sentences according to the class definition described below:\n",
    "\n",
    "- Class [0]: The entrepreneur gives (or has given) more weight to elements like chemistry, trust and personal bonding when evaluating a prospective cofounder.\n",
    "\n",
    "- Class [1]: The entrepreneur gives (or has given) more weight to elements like technical competences, past experience and execution capabilities when evaluating a prospective cofounder.\n",
    "\n",
    "- Class [2]: The entrepreneur looks (or has looked) for connections with whom she has strong ties when looking out for a potential cofounder (Serendipitous FOCIs).\n",
    "\n",
    "- Class [3]: The entrepreneur actively looks (or has looked) within her professional networks and/or engages in a dedicated search when looking out for a potential cofounder (Goal-directed FOCIs).\n",
    "\n",
    "These classes were developed taking into account propositions #1, #2, #3 and #4 as described in the research paper, narrrowing down our focus to the process followed by founders when they \"search for and mutually evaluate\" one another. Each sentence uploaded into the training set was carefully labelled to accurately represent the most appopriate class. Given that some sentences could be categorized into two different classes based on subjective claims by the human annotator (i.e. the sentence \"Evaluate whether you are able to work well together\" may be classified as both [0] and [1]), we estimate a human error level of ≈ 10%. \n",
    "\n",
    "So far, the model has been able to correctly identify ≈ 100% of the examples in the training set (n = 938), and ≈ 75% of the examples in the test set (n = 44, accounting for data the model has never \"seen\"), which implies some work has yet to be done to chop down overfitting.  \n",
    "\n",
    "This model has the potential to provide a reliable framework upon which experiments at big scales can be conducted, looking to test the principal hypotheses and prepositions exposed by the conceptual framework. Concretely, the model could help to advance the theoretical research developed by Prof. Clough and Prof. Vissa by using an NLP model to systematically web-scrape, classify and spot patterns among founder testimonies and evaluate them according to the stated prepositions.\n",
    "\n",
    "Let's take a deeper look at how the LSTM Neural Network model is structured. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading key Python libraries and Deep Learning Frameworks (Keras):\n",
    "\n",
    "import numpy as np\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Dropout, LSTM, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.initializers import glorot_uniform\n",
    "from utils_v1 import *\n",
    "\n",
    "# Loading of data files: (explain here the glove 6B model)\n",
    "\n",
    "word_to_index, index_to_word, word_to_vec_map = read_glove_vecs('data/glove.6B.50d.txt')\n",
    "\n",
    "X, Y = read_csv('data/sentences_data_v5.csv')\n",
    "X_train, Y_train = read_csv('data/sentences_data_train_v5.csv')\n",
    "X_test, Y_test = read_csv('data/sentences_data_test_v5.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data cleansing process - eliminating characters that may cause a KeyError message below:\n",
    "\n",
    "X[0] = X[0].replace('\\ufeff', '')\n",
    "X_train[0] = X_train[0].replace('\\ufeff', '')\n",
    "X_test[0] = X_test[0].replace('\\ufeff', '')\n",
    "\n",
    "# Finding out the max length of any given sentence in the dataset (counting in words) and storing the value in a variable called maxLen\n",
    "\n",
    "maxLen = len(max(X, key=len).split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentences_to_indices(X, word_to_index, max_len):\n",
    "    \"\"\"\n",
    "    Converts an array of sentences (strings) into an array of indices corresponding to words in the sentences.\n",
    "    The output shape should be such that it can be given to `Embedding()`\n",
    "    \n",
    "    Arguments:\n",
    "    X -- array of sentences (strings), of shape (m, 1)\n",
    "    word_to_index -- a dictionary containing all words mapped to their corresponding indeces\n",
    "    max_len -- maximum number of words in a sentence. Gets plugged as maxLen into the function. \n",
    "    \n",
    "    Returns:\n",
    "    X_indices -- array of indices corresponding to words in the sentences from X, of shape (m, max_len)\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[0]                                   # number of training examples\n",
    "    \n",
    "    # Initialize X_indices as a numpy matrix of zeros and the correct shape (≈ 1 line)\n",
    "    X_indices = np.zeros((m, max_len))\n",
    "    \n",
    "    for i in range(m):                               # loop over training examples\n",
    "        \n",
    "        # Convert the ith training sentence in lower case and split is into words. You should get a list of words.\n",
    "        sentence_words = [word.lower().replace('\\t', '') for word in X[i].split(' ') if word.replace('\\t', '') != '']\n",
    "        \n",
    "        # Initialize j to 0\n",
    "        j = 0\n",
    "        \n",
    "        # Loop over the words of sentence_words\n",
    "        for w in sentence_words:\n",
    "            # Set the (i,j)th entry of X_indices to the index of the correct word.\n",
    "            X_indices[i, j] = word_to_index[w]\n",
    "            # Increment j to j + 1\n",
    "            j = j + 1\n",
    "    \n",
    "    return X_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[185457. 170714. 209675. ...      0.      0.      0.]\n",
      " [368321. 268046. 254258. ...      0.      0.      0.]\n",
      " [ 56041. 103640.  54718. ...      0.      0.      0.]\n",
      " ...\n",
      " [366254. 239792. 335202. ...      0.      0.      0.]\n",
      " [224573. 259594. 141855. ...      0.      0.      0.]\n",
      " [357640.  58997. 233708. ...      0.      0.      0.]]\n"
     ]
    }
   ],
   "source": [
    "indices = sentences_to_indices(X, word_to_index, 18)\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrained_embedding_layer(word_to_vec_map, word_to_index):\n",
    "    \"\"\"\n",
    "    Creates a Keras Embedding() layer and loads in pre-trained GloVe 50-dimensional vectors.\n",
    "    \n",
    "    Arguments:\n",
    "    word_to_vec_map -- dictionary mapping words to their GloVe vector representation.\n",
    "    word_to_index -- dictionary mapping from words to their indices in the vocabulary (400,001 words)\n",
    "\n",
    "    Returns:\n",
    "    embedding_layer -- pretrained layer Keras instance\n",
    "    \"\"\"\n",
    "    \n",
    "    vocab_len = len(word_to_index) + 1                    # adding 1 to fit Keras embedding (requirement)\n",
    "    emb_dim = word_to_vec_map[\"co-founder\"].shape[0]      # define dimensionality of your GloVe word vectors (= 50)\n",
    "    \n",
    "    # Initializes the embedding matrix as a numpy array of zeros of shape (vocab_len, dimensions of word vectors = emb_dim)\n",
    "    emb_matrix = np.zeros((vocab_len, emb_dim))\n",
    "    \n",
    "    # Set each row \"index\" of the embedding matrix to be the word vector representation of the \"index\"th word of the vocabulary\n",
    "    for word, index in word_to_index.items():\n",
    "        emb_matrix[index, :] = word_to_vec_map[word]\n",
    "\n",
    "    # Defines the Keras embedding layer with the correct output/input sizes\n",
    "    embedding_layer = Embedding(vocab_len, emb_dim, trainable = True)\n",
    "\n",
    "    # Build the embedding layer, it is required before setting the weights of the embedding layer.\n",
    "    embedding_layer.build((None,))\n",
    "    \n",
    "    # Set the weights of the embedding layer to the embedding matrix. The layer should now be pretrained.\n",
    "    embedding_layer.set_weights([emb_matrix])\n",
    "    \n",
    "    return embedding_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NLP_model_v1(input_shape, word_to_vec_map, word_to_index):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function creating the NLP model's graph.\n",
    "    \n",
    "    Arguments:\n",
    "    input_shape -- shape of the input, usually (max_len,)\n",
    "    word_to_vec_map -- dictionary mapping every word in a vocabulary into its 50-dimensional vector representation\n",
    "    word_to_index -- dictionary mapping from words to their indices in the vocabulary (400,001 words)\n",
    "\n",
    "    Returns:\n",
    "    model -- a model instance in Keras\n",
    "    \"\"\"\n",
    "    \n",
    "    # Defines sentence_indices as the input of the graph, it should be of shape input_shape and dtype 'int32' (as it contains indices)\n",
    "    sentence_indices = Input(shape = input_shape, dtype = 'int32')\n",
    "    \n",
    "    # Creates the embedding layer pretrained with GloVe Vectors\n",
    "    embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)\n",
    "    \n",
    "    # Propagates sentence_indices through the embedding layer, getting back the embeddings\n",
    "    embeddings = embedding_layer(sentence_indices)  \n",
    "    \n",
    "    # Propagates the embeddings through an LSTM layer with 128-dimensional hidden state, setting return_sequences = True to get back a batch of sequences\n",
    "    X = LSTM(70, return_sequences = True)(embeddings)\n",
    "    \n",
    "    # Adds dropout with a probability of 0.5\n",
    "    X = Dropout(0.5)(X)\n",
    "    \n",
    "    # Propagates X trough another LSTM layer with 128-dimensional hidden state, this time with the returned output as a single hidden state, not a batch of sequences\n",
    "    X = LSTM(70, return_sequences = False)(X)\n",
    "    \n",
    "    # Adds dropout with a probability of 0.5\n",
    "    X = Dropout(0.5)(X)\n",
    "    \n",
    "    # Propagates X through a Dense layer with 4 units to get back a batch of 4-dimensional vectors\n",
    "    X = Dense(4, activation = None)(X)\n",
    "    \n",
    "    # Adds a softmax activation to estimate the probablity of each output \n",
    "    X = Activation('softmax')(X)\n",
    "    \n",
    "    # Creates the Model instance which converts sentence_indices into X.\n",
    "    model_v1 = Model(sentence_indices, X)\n",
    "    \n",
    "    return model_v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_7 (InputLayer)         (None, 18)                0         \n",
      "_________________________________________________________________\n",
      "embedding_7 (Embedding)      (None, 18, 50)            20000050  \n",
      "_________________________________________________________________\n",
      "lstm_13 (LSTM)               (None, 18, 70)            33880     \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 18, 70)            0         \n",
      "_________________________________________________________________\n",
      "lstm_14 (LSTM)               (None, 70)                39480     \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 70)                0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 4)                 284       \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 4)                 0         \n",
      "=================================================================\n",
      "Total params: 20,073,694\n",
      "Trainable params: 20,073,694\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_v1 = NLP_model_v1((18,), word_to_vec_map, word_to_index)\n",
    "model_v1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_v1.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_indices = sentences_to_indices(X_train, word_to_index, 18)\n",
    "Y_train_oh = convert_to_one_hot(Y_train, C = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "938/938 [==============================] - 14s 15ms/step - loss: 1.3652 - acc: 0.3134\n",
      "Epoch 2/20\n",
      "938/938 [==============================] - 9s 10ms/step - loss: 1.1839 - acc: 0.4744\n",
      "Epoch 3/20\n",
      "938/938 [==============================] - 9s 10ms/step - loss: 1.0119 - acc: 0.5448\n",
      "Epoch 4/20\n",
      "938/938 [==============================] - 9s 10ms/step - loss: 0.8628 - acc: 0.6493\n",
      "Epoch 5/20\n",
      "938/938 [==============================] - 9s 10ms/step - loss: 0.7744 - acc: 0.6844\n",
      "Epoch 6/20\n",
      "938/938 [==============================] - 9s 10ms/step - loss: 0.6806 - acc: 0.7292\n",
      "Epoch 7/20\n",
      "938/938 [==============================] - 9s 10ms/step - loss: 0.6172 - acc: 0.7644\n",
      "Epoch 8/20\n",
      "938/938 [==============================] - 10s 10ms/step - loss: 0.5336 - acc: 0.7921\n",
      "Epoch 9/20\n",
      "938/938 [==============================] - 9s 10ms/step - loss: 0.4355 - acc: 0.8497\n",
      "Epoch 10/20\n",
      "938/938 [==============================] - 9s 10ms/step - loss: 0.3574 - acc: 0.8721\n",
      "Epoch 11/20\n",
      "938/938 [==============================] - 9s 10ms/step - loss: 0.3729 - acc: 0.8678\n",
      "Epoch 12/20\n",
      "938/938 [==============================] - 9s 10ms/step - loss: 0.3316 - acc: 0.8902\n",
      "Epoch 13/20\n",
      "938/938 [==============================] - 9s 10ms/step - loss: 0.2377 - acc: 0.9243\n",
      "Epoch 14/20\n",
      "938/938 [==============================] - 9s 10ms/step - loss: 0.2230 - acc: 0.9243\n",
      "Epoch 15/20\n",
      "938/938 [==============================] - 9s 10ms/step - loss: 0.1805 - acc: 0.9488\n",
      "Epoch 16/20\n",
      "938/938 [==============================] - 10s 10ms/step - loss: 0.1830 - acc: 0.9510\n",
      "Epoch 17/20\n",
      "938/938 [==============================] - 10s 11ms/step - loss: 0.1468 - acc: 0.9488\n",
      "Epoch 18/20\n",
      "938/938 [==============================] - 9s 10ms/step - loss: 0.1358 - acc: 0.9627\n",
      "Epoch 19/20\n",
      "938/938 [==============================] - 9s 10ms/step - loss: 0.1024 - acc: 0.9701\n",
      "Epoch 20/20\n",
      "938/938 [==============================] - 9s 10ms/step - loss: 0.0654 - acc: 0.9872\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a33397390>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_v1.fit(X_train_indices, Y_train_oh, epochs = 20, batch_size = 32, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44/44 [==============================] - 1s 24ms/step\n",
      "\n",
      "Test_v1 accuracy =  0.7727272727272727\n"
     ]
    }
   ],
   "source": [
    "X_test_indices = sentences_to_indices(X_test, word_to_index, max_len = 18)\n",
    "Y_test_oh = convert_to_one_hot(Y_test, C = 4)\n",
    "loss_v1, acc_v1 = model_v1.evaluate(X_test_indices, Y_test_oh)\n",
    "print()\n",
    "print(\"Test_v1 accuracy = \", acc_v1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n",
      "[[9.7680032e-01 2.2174053e-02 5.9110031e-04 4.3462371e-04]]\n"
     ]
    }
   ],
   "source": [
    "x_test = np.array(['Chemistry for me is a key attribute'])\n",
    "X_test_indices = sentences_to_indices(x_test, word_to_index, 18)\n",
    "test_v1a = np.argmax(model_v1.predict(X_test_indices), axis = 1)\n",
    "test_v1b = model_v1.predict(X_test_indices)\n",
    "print(test_v1a)\n",
    "print(test_v1b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3]\n",
      "[[5.6452940e-05 2.3548992e-04 6.6381891e-04 9.9904424e-01]]\n"
     ]
    }
   ],
   "source": [
    "x_test = np.array(['I joined an accelerator program looking for a cofounder'])\n",
    "X_test_indices = sentences_to_indices(x_test, word_to_index, 18)\n",
    "test_v1a = np.argmax(model_v1.predict(X_test_indices), axis = 1)\n",
    "test_v1b = model_v1.predict(X_test_indices)\n",
    "print(test_v1a)\n",
    "print(test_v1b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n",
      "[[1.3849834e-02 9.8500812e-01 2.9611020e-04 8.4588368e-04]]\n"
     ]
    }
   ],
   "source": [
    "x_test = np.array(['I think she will make a good work'])\n",
    "X_test_indices = sentences_to_indices(x_test, word_to_index, 18)\n",
    "test_v1a = np.argmax(model_v1.predict(X_test_indices), axis = 1)\n",
    "test_v1b = model_v1.predict(X_test_indices)\n",
    "print(test_v1a)\n",
    "print(test_v1b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n",
      "[[1.0689745e-03 9.9868280e-01 4.8668378e-05 1.9958410e-04]]\n"
     ]
    }
   ],
   "source": [
    "x_test = np.array(['My cofounder should be able to execute the companys vision'])\n",
    "X_test_indices = sentences_to_indices(x_test, word_to_index, 18)\n",
    "test_v1a = np.argmax(model_v1.predict(X_test_indices), axis = 1)\n",
    "test_v1b = model_v1.predict(X_test_indices)\n",
    "print(test_v1a)\n",
    "print(test_v1b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n",
      "[[9.9549210e-01 2.0997690e-03 2.1668640e-03 2.4129801e-04]]\n"
     ]
    }
   ],
   "source": [
    "x_test = np.array(['My spouse and I founded a startup together'])\n",
    "X_test_indices = sentences_to_indices(x_test, word_to_index, 18)\n",
    "test_v1a = np.argmax(model_v1.predict(X_test_indices), axis = 1)\n",
    "test_v1b = model_v1.predict(X_test_indices)\n",
    "print(test_v1a)\n",
    "print(test_v1b)"
   ]
  }
